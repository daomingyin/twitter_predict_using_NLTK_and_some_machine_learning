{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I \n",
    "## Raw Data Process, Word2Vec and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Read Twitter csv file and compress the data to one-line-each-day format (concatenate all tweets within one day to a single string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/20</td>\n",
       "      <td>16:05:54</td>\n",
       "      <td>El M*gnici*io es la Õ_nica opciÕ_n para Venezu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/20</td>\n",
       "      <td>16:05:03</td>\n",
       "      <td>plying to @ironorehopper\\rInteresting, further...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/20</td>\n",
       "      <td>15:41:02</td>\n",
       "      <td>Meu Deus, tÕ£o inventando nome pra tudo \"White...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/20</td>\n",
       "      <td>14:59:53</td>\n",
       "      <td>Yo a ti conozco en persona , y eres el q se ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/20</td>\n",
       "      <td>14:42:13</td>\n",
       "      <td>Soy balsa , es mi saludo y yo saludo a quien q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381679</th>\n",
       "      <td>1/31/20</td>\n",
       "      <td>18:49:43</td>\n",
       "      <td>#Coronavirus  https://twitter.com/OSWALDORIOSM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381680</th>\n",
       "      <td>1/31/20</td>\n",
       "      <td>18:49:43</td>\n",
       "      <td>Coronavirus: What are the symptoms?  https://w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381681</th>\n",
       "      <td>1/31/20</td>\n",
       "      <td>18:49:43</td>\n",
       "      <td>Chicago PMIs &gt; coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381682</th>\n",
       "      <td>1/31/20</td>\n",
       "      <td>18:49:43</td>\n",
       "      <td>Como bolsonaro Ì© serviÌ¤al americano, brasile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381683</th>\n",
       "      <td>1/31/20</td>\n",
       "      <td>18:49:42</td>\n",
       "      <td>Disso aqui Ì©q eu tenho medo, nem lembro de co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>381684 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date      time                                              tweet\n",
       "0        1/1/20  16:05:54  El M*gnici*io es la Õ_nica opciÕ_n para Venezu...\n",
       "1        1/1/20  16:05:03  plying to @ironorehopper\\rInteresting, further...\n",
       "2        1/1/20  15:41:02  Meu Deus, tÕ£o inventando nome pra tudo \"White...\n",
       "3        1/1/20  14:59:53  Yo a ti conozco en persona , y eres el q se ga...\n",
       "4        1/1/20  14:42:13  Soy balsa , es mi saludo y yo saludo a quien q...\n",
       "...         ...       ...                                                ...\n",
       "381679  1/31/20  18:49:43  #Coronavirus  https://twitter.com/OSWALDORIOSM...\n",
       "381680  1/31/20  18:49:43  Coronavirus: What are the symptoms?  https://w...\n",
       "381681  1/31/20  18:49:43                         Chicago PMIs > coronavirus\n",
       "381682  1/31/20  18:49:43  Como bolsonaro Ì© serviÌ¤al americano, brasile...\n",
       "381683  1/31/20  18:49:42  Disso aqui Ì©q eu tenho medo, nem lembro de co...\n",
       "\n",
       "[381684 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read twitter csv file\n",
    "tw = pd.read_csv('corona.csv', encoding = \"ISO-8859-1\")\n",
    "tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1/1/20',\n",
       " '1/10/20',\n",
       " '1/11/20',\n",
       " '1/12/20',\n",
       " '1/13/20',\n",
       " '1/14/20',\n",
       " '1/15/20',\n",
       " '1/16/20',\n",
       " '1/17/20',\n",
       " '1/18/20',\n",
       " '1/19/20',\n",
       " '1/2/20',\n",
       " '1/20/20',\n",
       " '1/21/20',\n",
       " '1/22/20',\n",
       " '1/23/20',\n",
       " '1/24/20',\n",
       " '1/25/20',\n",
       " '1/26/20',\n",
       " '1/27/20',\n",
       " '1/28/20',\n",
       " '1/29/20',\n",
       " '1/3/20',\n",
       " '1/30/20',\n",
       " '1/31/20',\n",
       " '1/4/20',\n",
       " '1/5/20',\n",
       " '1/6/20',\n",
       " '1/7/20',\n",
       " '1/8/20',\n",
       " '1/9/20',\n",
       " '2/1/20',\n",
       " '2/10/20',\n",
       " '2/11/20',\n",
       " '2/12/20',\n",
       " '2/13/20',\n",
       " '2/14/20',\n",
       " '2/15/20',\n",
       " '2/16/20',\n",
       " '2/17/20',\n",
       " '2/18/20',\n",
       " '2/19/20',\n",
       " '2/2/20',\n",
       " '2/20/20',\n",
       " '2/21/20',\n",
       " '2/22/20',\n",
       " '2/23/20',\n",
       " '2/24/20',\n",
       " '2/25/20',\n",
       " '2/26/20',\n",
       " '2/27/20',\n",
       " '2/28/20',\n",
       " '2/29/20',\n",
       " '2/3/20',\n",
       " '2/4/20',\n",
       " '2/5/20',\n",
       " '2/6/20',\n",
       " '2/7/20',\n",
       " '2/8/20',\n",
       " '2/9/20',\n",
       " '3/1/20',\n",
       " '3/10/20',\n",
       " '3/11/20',\n",
       " '3/12/20',\n",
       " '3/13/20',\n",
       " '3/14/20',\n",
       " '3/15/20',\n",
       " '3/16/20',\n",
       " '3/17/20',\n",
       " '3/18/20',\n",
       " '3/19/20',\n",
       " '3/2/20',\n",
       " '3/20/20',\n",
       " '3/21/20',\n",
       " '3/22/20',\n",
       " '3/23/20',\n",
       " '3/24/20',\n",
       " '3/25/20',\n",
       " '3/26/20',\n",
       " '3/27/20',\n",
       " '3/28/20',\n",
       " '3/29/20',\n",
       " '3/3/20',\n",
       " '3/30/20',\n",
       " '3/31/20',\n",
       " '3/4/20',\n",
       " '3/5/20',\n",
       " '3/6/20',\n",
       " '3/7/20',\n",
       " '3/8/20',\n",
       " '3/9/20',\n",
       " '4/1/20',\n",
       " '4/10/20',\n",
       " '4/11/20',\n",
       " '4/12/20',\n",
       " '4/13/20',\n",
       " '4/14/20',\n",
       " '4/15/20',\n",
       " '4/16/20',\n",
       " '4/17/20',\n",
       " '4/18/20',\n",
       " '4/19/20',\n",
       " '4/2/20',\n",
       " '4/20/20',\n",
       " '4/21/20',\n",
       " '4/22/20',\n",
       " '4/23/20',\n",
       " '4/24/20',\n",
       " '4/25/20',\n",
       " '4/3/20',\n",
       " '4/4/20',\n",
       " '4/5/20',\n",
       " '4/6/20',\n",
       " '4/7/20',\n",
       " '4/8/20',\n",
       " '4/9/20'}"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique date values in order to locate the tweets within same day\n",
    "dates = tw.date\n",
    "dates_unique = set(dates)\n",
    "dates_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>El M*gnici*io es la Õ_nica opciÕ_n para Venezu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>Sim, mas nas redes sociais ninguÕ©m precisa pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>A pan-coronavirus fusion inhibitor targeting t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>Estados Unidos pide a Rep. Dom. que le mande a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>TÕ no papo kkkkskskks. \\nEnero el mes de las ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>CoronavÕ_rus: Brasil registra primeira morte d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>#Coronavirus: un colegio de Laboulaye le deja ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-23</th>\n",
       "      <td>With the scarcity of #foodservice jobs availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>As colleges and schools scramble to take their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>\"It's not about if we're going to get it, it's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       tweets\n",
       "dates                                                        \n",
       "2020-01-01  El M*gnici*io es la Õ_nica opciÕ_n para Venezu...\n",
       "2020-01-02  Sim, mas nas redes sociais ninguÕ©m precisa pe...\n",
       "2020-01-03  A pan-coronavirus fusion inhibitor targeting t...\n",
       "2020-01-04  Estados Unidos pide a Rep. Dom. que le mande a...\n",
       "2020-01-05  TÕ no papo kkkkskskks. \\nEnero el mes de las ...\n",
       "...                                                       ...\n",
       "2020-04-21  CoronavÕ_rus: Brasil registra primeira morte d...\n",
       "2020-04-22  #Coronavirus: un colegio de Laboulaye le deja ...\n",
       "2020-04-23  With the scarcity of #foodservice jobs availab...\n",
       "2020-04-24  As colleges and schools scramble to take their...\n",
       "2020-04-25  \"It's not about if we're going to get it, it's...\n",
       "\n",
       "[116 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed = pd.DataFrame({'dates':[], 'tweets':[]})\n",
    "for i in dates_unique:         # for each date: \n",
    "    df = tw[tw['date']==i]     # get a dataframe containing all tweets in that day\n",
    "    twt_combined = ''          # make up a string to store tweet contents\n",
    "    for j in df['tweet']:\n",
    "        twt_combined = twt_combined+j        \n",
    "        twt_combined = twt_combined+'. \\n'\n",
    "    df_compressed = pd.DataFrame({'dates':[i], 'tweets':[twt_combined]})\n",
    "    compressed = pd.concat([compressed, df_compressed])\n",
    "compressed['dates'] = pd.to_datetime(compressed['dates'])\n",
    "compressed.sort_values(by = 'dates', inplace = True)\n",
    "compressed.set_index('dates', inplace = True)\n",
    "compressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2) Use Word2Vec model to train WSJ files and get coronavirus related library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rapid', 'viral', 'covid', 'coronavirus', 'slow', 'asymptomat', 'flu', 'sar', 'ebola', 'influenza', 'epicent', 'summari', 'virus']\n",
      "['sar', 'dead', 'contagion', 'respiratori']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec #to install gensim, open powershell prompt, and type in: conda install -c anaconda gensim\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import csv     \n",
    "\n",
    "class MySentences(object):      #this function helps us read all of the txt files in the designated folder\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname),encoding = \"ISO-8859-1\"):\n",
    "                yield line.split(\"', '\")\n",
    "\n",
    "def w2v(text):      #this function trains the word2vector model (output) using text documents (input)\n",
    "    model_w2v = Word2Vec(text)\n",
    "    return model_w2v\n",
    "\n",
    "def Convert(lst):     #this function converts a list of list (input) into a dictionary (output)\n",
    "    it = iter(lst)\n",
    "    res_dct = dict(it)\n",
    "    return res_dct\n",
    "\n",
    "def find_similar(word,model_w2v):   #this function returns the expanded dictionary (most similar words to the seed words)\n",
    "    word_dict = model_w2v.wv.most_similar(positive=[word],topn=20)   \n",
    "    d = Convert(word_dict)\n",
    "    return d\n",
    "\n",
    "def get_seedword(keyword,dict):\n",
    "    return dict.get(keyword)\n",
    "\n",
    "\n",
    "def freq(model):\n",
    "    freq_dict = {}\n",
    "    for word in model.wv.vocab:\n",
    "        freq_dict.update({word: model.wv.vocab[word].count})\n",
    "    return freq_dict\n",
    "\n",
    "def main():\n",
    "    curDir = os.getcwd()  \n",
    "    os.chdir('/Users/liwenqian/Documents') \n",
    "    sentences = MySentences('./wsj')\n",
    "    model_w2v = w2v(sentences)\n",
    "    model_w2v.save(\"word2vec.model\")\n",
    "    freq_dict = freq(model_w2v)\n",
    "    \n",
    "    sorted_x = sorted(freq_dict.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    sorted_dict = collections.OrderedDict(sorted_x)\n",
    "    with open(\"frequencies.csv\", \"w\") as csv_file:\n",
    "        csvwriter = csv.writer(csv_file)\n",
    "        for item in sorted_dict:\n",
    "            csvwriter.writerow([item,sorted_dict[item]])\n",
    "\n",
    "\n",
    "    #seed words:\n",
    "    category = {'virus': ['virus','wuhan','covid','sick','coronavirus'],\n",
    "                'pandemic': ['outbreak','transmiss','symptom','remdesivir'],\n",
    "                'test': ['test','index','stay'],\n",
    "                'mask': ['mask','droplet','flatten'],\n",
    "                'health': ['health','cure'],\n",
    "                'social': ['social','contract','law'],\n",
    "                'market': ['market','uncertain','risk','plummet','circuit']}\n",
    "    \n",
    "    keywords = []\n",
    "    for item in category.keys():\n",
    "        keywords.append(item)\n",
    "    keyword_dict = {}\n",
    "    for keyword in keywords:\n",
    "        seedword_dict = {}\n",
    "        seedwords = get_seedword(keyword,category)\n",
    "        for seedword in seedwords:\n",
    "            sim_dict = find_similar(seedword,model_w2v)\n",
    "            seedword_dict.update({seedword: sim_dict})\n",
    "        keyword_dict.update({keyword: seedword_dict})\n",
    "\n",
    "    #Remove all the repeated words that do not have the highest probability\n",
    "    update_dict = dict(keyword_dict) #copy dictionary from above\n",
    "    for keyword in keyword_dict.keys():\n",
    "        lst = [] #create list of words for the whole dictionary\n",
    "        for seedword in keyword_dict[keyword]:\n",
    "            for item in keyword_dict[keyword][seedword]:\n",
    "                lst.append(item)\n",
    "        cnt = Counter(lst) #count the words frequencies\n",
    "        repeat_item = [x for x, y in cnt.items() if y > 1] #find repeated words\n",
    "        print(repeat_item)\n",
    "        for word in repeat_item:\n",
    "            repeat_dict = {} #create probability dictionary for each word in the form {probability1:word,probability2:word...}\n",
    "            for seedword in keyword_dict[keyword]:\n",
    "                if word in list(keyword_dict[keyword][seedword].keys()):\n",
    "                    repeat_dict.update({keyword_dict[keyword][seedword][word]: word})\n",
    "            prob = [key for key in repeat_dict.keys()] #get probabilities of all repeated words\n",
    "            max_prob = max(prob)\n",
    "            for seedword in keyword_dict[keyword]:\n",
    "                if word in list(keyword_dict[keyword][seedword].keys()):\n",
    "                    if max_prob and keyword_dict[keyword][seedword][word] != max_prob:\n",
    "                        del update_dict[keyword][seedword][word] #delete the words that do not have maximum probability\n",
    "\n",
    "    with open('updated_dict.csv', 'w') as csv_file:\n",
    "        csvwriter = csv.writer(csv_file, delimiter='\\t')\n",
    "        for keyword in update_dict:\n",
    "            for seedword in update_dict[keyword]:\n",
    "                csvwriter.writerow([keyword, ',', seedword, ',', list(update_dict[keyword][seedword].keys())])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['highest',\n",
       " 'ill',\n",
       " 'selfisol',\n",
       " 'stem',\n",
       " 'toll',\n",
       " 'sar',\n",
       " 'pneumonia',\n",
       " 'antivir',\n",
       " 'sampl',\n",
       " 'seng',\n",
       " 'stuck',\n",
       " 'glove',\n",
       " 'approx',\n",
       " 'though',\n",
       " 'mount',\n",
       " 'persist',\n",
       " 'physic',\n",
       " 'signific',\n",
       " 'regul',\n",
       " 'bond',\n",
       " 'grief',\n",
       " 'transmiss',\n",
       " 'yuan',\n",
       " 'cathedr',\n",
       " 'spain',\n",
       " 'flu',\n",
       " 'leav',\n",
       " 'orlean',\n",
       " 'pandem',\n",
       " 'pathogen',\n",
       " 'suspect',\n",
       " 'antimalari',\n",
       " 'posit',\n",
       " 'yield',\n",
       " 'famili',\n",
       " 'face',\n",
       " 'cling',\n",
       " 'uncheck',\n",
       " 'emerg',\n",
       " 'lifethreaten',\n",
       " 'recommend',\n",
       " 'increas',\n",
       " 'assist',\n",
       " 'profit',\n",
       " 'prospect',\n",
       " 'serious',\n",
       " 'slid',\n",
       " 'coowner',\n",
       " 'provinc',\n",
       " 'influenza',\n",
       " 'worri',\n",
       " 'handl',\n",
       " 'spread',\n",
       " 'relat',\n",
       " 'expos',\n",
       " 'arthriti',\n",
       " 'symptom',\n",
       " 'sale',\n",
       " 'work',\n",
       " 'gown',\n",
       " 'gust',\n",
       " 'unknow',\n",
       " 'intens',\n",
       " 'complic',\n",
       " 'distanc',\n",
       " 'ravag',\n",
       " 'guidanc',\n",
       " 'forecast',\n",
       " 'starter',\n",
       " 'becom',\n",
       " 'fastest',\n",
       " 'tick',\n",
       " 'deadliest',\n",
       " 'mild',\n",
       " 'someon',\n",
       " 'escal',\n",
       " 'itali',\n",
       " 'bat',\n",
       " 'die',\n",
       " 'antiinflammatori',\n",
       " 'identifi',\n",
       " 'profit',\n",
       " 'selfisol',\n",
       " 'goggl',\n",
       " 'aerosol',\n",
       " 'catch',\n",
       " 'barton',\n",
       " 'recurr',\n",
       " 'approx',\n",
       " 'hemorrhag',\n",
       " 'general',\n",
       " 'big',\n",
       " 'pain',\n",
       " 'danger',\n",
       " 'slate',\n",
       " 'arctic',\n",
       " 'jan',\n",
       " 'ebola',\n",
       " 'volunt',\n",
       " 'plateau',\n",
       " 'contain',\n",
       " 'factor',\n",
       " 'symptomat',\n",
       " 'rheumatoid',\n",
       " 'negat',\n",
       " 'carinsur',\n",
       " 'funer',\n",
       " 'gear',\n",
       " 'particl',\n",
       " 'becom',\n",
       " 'organ',\n",
       " 'reappear',\n",
       " 'turnstil',\n",
       " 'alreadi',\n",
       " 'feder',\n",
       " 'growth',\n",
       " 'this',\n",
       " 'persontoperson',\n",
       " 'yearearli',\n",
       " 'cuando',\n",
       " 'uk',\n",
       " 'pneumonia',\n",
       " 'without',\n",
       " 'rochell',\n",
       " 'wuhan',\n",
       " 'lung',\n",
       " 'sampl',\n",
       " 'malaria',\n",
       " 'pcr',\n",
       " 'annual',\n",
       " 'visit',\n",
       " 'cloth',\n",
       " 'especi',\n",
       " 'persontoperson',\n",
       " 'immigr',\n",
       " 'danger',\n",
       " 'maintain',\n",
       " 'spike',\n",
       " 'affair',\n",
       " 'sale',\n",
       " 'goe',\n",
       " 'contagi',\n",
       " 'minus',\n",
       " 'basin',\n",
       " 'fatal',\n",
       " 'nadia',\n",
       " 'isol',\n",
       " 'began',\n",
       " 'pummel',\n",
       " 'contagion',\n",
       " 'recov',\n",
       " 'generic',\n",
       " 'diagnost',\n",
       " 'barrel',\n",
       " 'apart',\n",
       " 'protect',\n",
       " 'surfac',\n",
       " 'true',\n",
       " 'abat',\n",
       " 'catch',\n",
       " 'strict',\n",
       " 'far',\n",
       " 'agenc',\n",
       " 'debt',\n",
       " 'tempt',\n",
       " 'older',\n",
       " 'outpac',\n",
       " 'freddi',\n",
       " 'death',\n",
       " 'suffer',\n",
       " 'wait',\n",
       " 'hn',\n",
       " 'uk',\n",
       " 'complic',\n",
       " 'presenc',\n",
       " 'hydroxychloroquin',\n",
       " 'diagnos',\n",
       " 'revenu',\n",
       " 'beutel',\n",
       " 'equip',\n",
       " 'nose',\n",
       " 'recurr',\n",
       " 'address',\n",
       " 'prophylaxi',\n",
       " 'stop',\n",
       " 'moreov',\n",
       " 'aid',\n",
       " 'recess',\n",
       " 'lone',\n",
       " 'elder',\n",
       " 'streak',\n",
       " 'avolon',\n",
       " 'epicent',\n",
       " 'sickest',\n",
       " 'child',\n",
       " 'lethal',\n",
       " 'particular',\n",
       " 'asymptomat',\n",
       " 'rheumatoidarthr',\n",
       " 'antibodi',\n",
       " 'slash',\n",
       " 'feet',\n",
       " 'medicalgrad',\n",
       " 'raw',\n",
       " 'dead',\n",
       " 'infectiousdiseas',\n",
       " 'milder',\n",
       " 'safe',\n",
       " 'disproportion',\n",
       " 'council',\n",
       " 'valu',\n",
       " 'habit',\n",
       " 'fear',\n",
       " 'histor',\n",
       " 'gun',\n",
       " 'talli',\n",
       " 'sever',\n",
       " 'intub',\n",
       " 'due',\n",
       " 'diabet',\n",
       " 'nadia',\n",
       " 'efficaci',\n",
       " 'criteria',\n",
       " 'earn',\n",
       " 'nurs',\n",
       " 'reus',\n",
       " 'minim',\n",
       " 'fear',\n",
       " 'stipul',\n",
       " 'contagi',\n",
       " 'hygien',\n",
       " 'caus',\n",
       " 'healthcaregov',\n",
       " 'downturn',\n",
       " 'imagin',\n",
       " 'factor',\n",
       " 'midpoint',\n",
       " 'chrysler',\n",
       " 'confirm',\n",
       " 'serious',\n",
       " 'symptomat',\n",
       " 'swept',\n",
       " 'unknown',\n",
       " 'cough',\n",
       " 'random',\n",
       " 'swab',\n",
       " 'forecast',\n",
       " 'parent',\n",
       " 'sanit',\n",
       " 'touch',\n",
       " 'battl',\n",
       " 'justic',\n",
       " 'protein',\n",
       " 'sake',\n",
       " 'resumpt',\n",
       " 'program',\n",
       " 'share',\n",
       " 'desir',\n",
       " 'transmit',\n",
       " 'soar',\n",
       " 'stanley',\n",
       " 'iran',\n",
       " 'identifi',\n",
       " 'anyon',\n",
       " 'combat',\n",
       " 'viral',\n",
       " 'isol',\n",
       " 'zpak',\n",
       " 'cepheid',\n",
       " 'drop',\n",
       " 'uninfect',\n",
       " 'wore',\n",
       " 'humid',\n",
       " 'carri',\n",
       " 'transport',\n",
       " 'molecul',\n",
       " 'unnecessari',\n",
       " 'carrier',\n",
       " 'issu',\n",
       " 'invest',\n",
       " 'somewher',\n",
       " 'common',\n",
       " 'onequart',\n",
       " 'sail',\n",
       " 'sar',\n",
       " 'detect',\n",
       " 'room',\n",
       " 'ketchum',\n",
       " 'known',\n",
       " 'flulik',\n",
       " 'leukin',\n",
       " 'inmat',\n",
       " 'factset',\n",
       " 'condren',\n",
       " 'latex',\n",
       " 'common',\n",
       " 'particular',\n",
       " 'judith',\n",
       " 'bone',\n",
       " 'crowd',\n",
       " 'unknow',\n",
       " 'overse',\n",
       " 'index',\n",
       " 'uncomfort',\n",
       " 'condit',\n",
       " 'twoyear',\n",
       " 'adida',\n",
       " 'hubei',\n",
       " 'person',\n",
       " 'persontoperson',\n",
       " 'someon',\n",
       " 'hiv',\n",
       " 'laboratori',\n",
       " 'bluechip',\n",
       " 'door',\n",
       " 'bandanna',\n",
       " 'duplic',\n",
       " 'experi',\n",
       " 'vascular',\n",
       " 'presenc',\n",
       " 'connect',\n",
       " 'sicken',\n",
       " 'committe',\n",
       " 'plung',\n",
       " 'deeper',\n",
       " 'pathogen',\n",
       " 'brent',\n",
       " 'yeezi',\n",
       " 'swine',\n",
       " 'donor',\n",
       " 'influenza',\n",
       " 'admit',\n",
       " 'offlabel',\n",
       " 'healthdepart',\n",
       " 'occident',\n",
       " 'bring',\n",
       " 'id',\n",
       " 'microb',\n",
       " 'resurg',\n",
       " 'be',\n",
       " 'clear',\n",
       " 'minim',\n",
       " 'similar',\n",
       " 'union',\n",
       " 'equiti',\n",
       " 'profound',\n",
       " 'under',\n",
       " 'precrisi',\n",
       " 'lo',\n",
       " 'occur',\n",
       " 'regardless',\n",
       " 'respiratori',\n",
       " 'keytruda',\n",
       " 'serolog',\n",
       " 'doubledigit',\n",
       " 'place',\n",
       " 'dispos',\n",
       " 'air',\n",
       " 'mitig',\n",
       " 'human',\n",
       " 'gene',\n",
       " 'activ',\n",
       " 'tie',\n",
       " 'enforc',\n",
       " 'interest',\n",
       " 'inconveni',\n",
       " 'suffer',\n",
       " 'third',\n",
       " 'debut']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the words in library because some words are with characters like []'\n",
    "updated_dict = pd.read_csv('updated_dict.csv')\n",
    "lib = []\n",
    "for i in updated_dict:\n",
    "    vec = updated_dict[i]\n",
    "    vec = vec.dropna()\n",
    "    for j in vec:\n",
    "        signs = \"'[] \"\n",
    "        for k in signs:\n",
    "            j = j.replace(k,'')\n",
    "        lib.append(j)\n",
    "lib   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Filter tweets for relevant contexts. For sentences with words in the library, keep that sentence. Otherwise drop that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_tweets = []\n",
    "signs = \"`~1234567890-=!@#$%^&*()_+[]\\{}|;':,./<>?\"\n",
    "for tweet in compressed['tweets']:        # for each day's tweet:\n",
    "    sentences = tweet.split('\\n')         # break into sentences,\n",
    "    tweet_new=[]\n",
    "    for i in sentences:                   # for each sentence:\n",
    "        i_spl = i.split(' ')              # break into tokens,\n",
    "        toks = []\n",
    "        for j in i_spl:\n",
    "            for k in signs:\n",
    "                j = j.replace(k, '')\n",
    "            toks.append(j)  \n",
    "    #     print(toks)\n",
    "        for m in toks:                    # iterate over tokens in that sentence:\n",
    "            m_stem = stemmer.stem(m)      \n",
    "            if m_stem in lib or m in lib: # once encounter a token or its stem that is in the lib,\n",
    "                tweet_new.append(i)       # keep that sentence, stop iterating that sentence,\n",
    "                break                     # and iterate next sentence.\n",
    "    new_tweet=''\n",
    "    for sent in tweet_new:                # concatenate tweets within same day into one single string\n",
    "        new_tweet = new_tweet + '\\n' + sent\n",
    "    new_tweet = new_tweet.strip()\n",
    "    relevant_tweets.append(new_tweet)     # get relevant tweets for each day and make into a data frame\n",
    "\n",
    "compressed_relevant = pd.DataFrame({'tweets':relevant_tweets}, index = compressed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>plying to @ironorehopper\\rInteresting, further...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>Ten que corijir mesmo, odÕ_io cuando mim ler a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>A pan-coronavirus fusion inhibitor targeting t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>Tengo imÕgenes que dicen lo contrario.. \\nSur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>Supposedly all the usual suspects ruled out. P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>CUANDO QUIERAS CORONAVIRUS PEDORRO!!!  https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>Are you out of work due to #coronavirus? Is yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-23</th>\n",
       "      <td>If the coronavirus response team were a band, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>As colleges and schools scramble to take their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>\"It's not about if we're going to get it, it's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       tweets\n",
       "dates                                                        \n",
       "2020-01-01  plying to @ironorehopper\\rInteresting, further...\n",
       "2020-01-02  Ten que corijir mesmo, odÕ_io cuando mim ler a...\n",
       "2020-01-03  A pan-coronavirus fusion inhibitor targeting t...\n",
       "2020-01-04  Tengo imÕgenes que dicen lo contrario.. \\nSur...\n",
       "2020-01-05  Supposedly all the usual suspects ruled out. P...\n",
       "...                                                       ...\n",
       "2020-04-21  CUANDO QUIERAS CORONAVIRUS PEDORRO!!!  https:/...\n",
       "2020-04-22  Are you out of work due to #coronavirus? Is yo...\n",
       "2020-04-23  If the coronavirus response team were a band, ...\n",
       "2020-04-24  As colleges and schools scramble to take their...\n",
       "2020-04-25  \"It's not about if we're going to get it, it's...\n",
       "\n",
       "[116 rows x 1 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4) Perform daily sentiment analysis and make record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liwenqian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define all relevant functions to be used for sentiment analysis\n",
    "import re    \n",
    "import os    \n",
    "import os.path\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer  #import tokenizer\n",
    "nltk.download('stopwords')  #download the list of stopwords, if you have not already done so\n",
    "from nltk.corpus import stopwords  #import the list of stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer  #import stemmer module\n",
    "import pandas as pd\n",
    "\n",
    "# Negation handling\n",
    "stemmer = SnowballStemmer('english')\n",
    "def negation_handling (text):\n",
    "    negatereg1=re.compile(r'n’t \\S*')\n",
    "    negatereg2=re.compile(r'no \\S*')\n",
    "    negatereg3=re.compile(r'not \\S*')\n",
    "    \n",
    "    negate_list1=negatereg1.findall(docu)\n",
    "    negate_list2=negatereg2.findall(docu)\n",
    "    negate_list3=negatereg3.findall(docu)\n",
    "    negate_list = negate_list1 + negate_list2 + negate_list3\n",
    "\n",
    "    text1 = re.sub(negatereg1,'', text) \n",
    "    text2 = re.sub(negatereg2,'', text1) \n",
    "    text3 = re.sub(negatereg3,'', text2) \n",
    "    \n",
    "    return text3, negate_list\n",
    "\n",
    "# remove punctuations, remove stopwords, stem and tokenize\n",
    "tokenizer = MWETokenizer([('new', 'york'), ('san', 'francisco')])\n",
    "stemmer = SnowballStemmer('english')\n",
    "def clean_tokenize (text):\n",
    "    # remove numbers\n",
    "    text_nonum = re.sub(r'\\d+', '', text)\n",
    "    text_nonum = text_nonum.replace('”', '')\n",
    "    text_nonum = text_nonum.replace('“', '')\n",
    "    text_nonum = text_nonum.replace('—', ' ')\n",
    "    # remove punctuations and convert characters to lower case\n",
    "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \n",
    "    # substitute multiple whitespace with single whitespace. Also, removes leading and trailing whitespaces\n",
    "    text_cleaned = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    text_output = tokenizer.tokenize(text_cleaned.split())\n",
    "    text_stopwords = []\n",
    "    for word in text_output:\n",
    "        if word not in stopwords.words('english'):  #filter the stop words\n",
    "            text_stopwords.append(word) \n",
    "    text_stemmed = ([stemmer.stem(w) for w in text_stopwords])\n",
    "    return text_stemmed\n",
    "\n",
    "# sentiment classification and construction of sentiment measures\n",
    "df1 = pd.read_excel('LM Sentiment Dictionary.xlsx', sheet_name='Positive')\n",
    "positive_temp = df1['WORD'].tolist()\n",
    "positive = [item.lower() for item in positive_temp]\n",
    "positive_stemmed = ([stemmer.stem(w) for w in positive])\n",
    "positive_negate = []\n",
    "for word in positive_stemmed:\n",
    "    positive_negate.append('n’t '+ word)\n",
    "    positive_negate.append('no '+ word)\n",
    "    positive_negate.append('not '+ word)\n",
    "\n",
    "df2 = pd.read_excel('LM Sentiment Dictionary.xlsx', sheet_name='Negative')  \n",
    "negative_temp = df2['WORD'].tolist()\n",
    "negative = [item.lower() for item in negative_temp]\n",
    "negative_stemmed = ([stemmer.stem(w) for w in negative])\n",
    "negative_negate = []\n",
    "for word in negative_stemmed:\n",
    "    negative_negate.append('n’t '+ word)\n",
    "    negative_negate.append('no '+ word)\n",
    "    negative_negate.append('not '+ word)\n",
    "\n",
    "positive_final = positive_stemmed + negative_negate\n",
    "negative_final = negative_stemmed + positive_negate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 00:00:00 -0.013623978201634877\n",
      "2020-01-02 00:00:00 -0.013513513513513514\n",
      "2020-01-03 00:00:00 -0.012\n",
      "2020-01-04 00:00:00 -0.002638522427440633\n",
      "2020-01-05 00:00:00 -0.01\n",
      "2020-01-06 00:00:00 -0.017761989342806393\n",
      "2020-01-07 00:00:00 -0.009259259259259259\n",
      "2020-01-08 00:00:00 -0.03417935702199661\n",
      "2020-01-09 00:00:00 -0.035932138772398016\n",
      "2020-01-10 00:00:00 -0.02071192313750197\n",
      "2020-01-11 00:00:00 -0.015017956252040483\n",
      "2020-01-12 00:00:00 -0.015471167369901548\n",
      "2020-01-13 00:00:00 -0.013071895424836602\n",
      "2020-01-14 00:00:00 -0.029275733123400906\n",
      "2020-01-15 00:00:00 -0.01881917020135537\n",
      "2020-01-16 00:00:00 -0.028658691779078645\n",
      "2020-01-17 00:00:00 -0.02727640995633974\n",
      "2020-01-18 00:00:00 -0.026542831549667627\n",
      "2020-01-19 00:00:00 -0.027964244721922715\n",
      "2020-01-20 00:00:00 -0.0163265306122449\n",
      "2020-01-21 00:00:00 -0.01838134947152444\n",
      "2020-01-22 00:00:00 -0.024015513126491646\n",
      "2020-01-23 00:00:00 -0.02865118069815195\n",
      "2020-01-24 00:00:00 -0.0223209068598808\n",
      "2020-01-25 00:00:00 -0.02385174853043738\n",
      "2020-01-26 00:00:00 -0.0232650363516193\n",
      "2020-01-27 00:00:00 -0.028108709846611648\n",
      "2020-01-28 00:00:00 -0.025891908097353575\n",
      "2020-01-29 00:00:00 -0.02593460432420918\n",
      "2020-01-30 00:00:00 -0.021995648991949633\n",
      "2020-01-31 00:00:00 -0.03128047408296452\n",
      "2020-02-01 00:00:00 -0.02730336305907705\n",
      "2020-02-02 00:00:00 -0.026853421357837453\n",
      "2020-02-03 00:00:00 -0.025605050859347597\n",
      "2020-02-04 00:00:00 -0.02760811140972392\n",
      "2020-02-05 00:00:00 -0.024112116991643454\n",
      "2020-02-06 00:00:00 -0.03303724481845099\n",
      "2020-02-07 00:00:00 -0.02453461439478425\n",
      "2020-02-08 00:00:00 -0.029260251390892233\n",
      "2020-02-09 00:00:00 -0.024310671437592087\n",
      "2020-02-10 00:00:00 -0.02541156606162938\n",
      "2020-02-11 00:00:00 -0.024719101123595506\n",
      "2020-02-12 00:00:00 -0.03372153658960542\n",
      "2020-02-13 00:00:00 -0.031672382489408915\n",
      "2020-02-14 00:00:00 -0.025339096735728126\n",
      "2020-02-15 00:00:00 -0.028025870033877427\n",
      "2020-02-16 00:00:00 -0.03172106435968017\n",
      "2020-02-17 00:00:00 -0.03191099156376085\n",
      "2020-02-18 00:00:00 -0.029261155815654718\n",
      "2020-02-19 00:00:00 -0.0321377331420373\n",
      "2020-02-20 00:00:00 -0.020147921448610048\n",
      "2020-02-21 00:00:00 -0.03473152633206225\n",
      "2020-02-22 00:00:00 -0.03376133608691748\n",
      "2020-02-23 00:00:00 -0.029626146249706088\n",
      "2020-02-24 00:00:00 -0.03655703631250764\n",
      "2020-02-25 00:00:00 -0.0320386016157956\n",
      "2020-02-26 00:00:00 -0.03217588224193244\n",
      "2020-02-27 00:00:00 -0.036990135963743\n",
      "2020-02-28 00:00:00 -0.03407851690294438\n",
      "2020-02-29 00:00:00 -0.0323511845510651\n",
      "2020-03-01 00:00:00 -0.030982341122977687\n",
      "2020-03-02 00:00:00 -0.030099594245665807\n",
      "2020-03-03 00:00:00 -0.0389937106918239\n",
      "2020-03-04 00:00:00 -0.02842771130249219\n",
      "2020-03-05 00:00:00 -0.0315252665739453\n",
      "2020-03-06 00:00:00 -0.029400168966488314\n",
      "2020-03-07 00:00:00 -0.02791706193251118\n",
      "2020-03-08 00:00:00 -0.03484517745977848\n",
      "2020-03-09 00:00:00 -0.03351561121890986\n",
      "2020-03-10 00:00:00 -0.028151716971328705\n",
      "2020-03-11 00:00:00 -0.032683235251203736\n",
      "2020-03-12 00:00:00 -0.028014987800627396\n",
      "2020-03-13 00:00:00 -0.022691663515046735\n",
      "2020-03-14 00:00:00 -0.030895260386190754\n",
      "2020-03-15 00:00:00 -0.02653627120665494\n",
      "2020-03-16 00:00:00 -0.030112991295885818\n",
      "2020-03-17 00:00:00 -0.028913311421528348\n",
      "2020-03-18 00:00:00 -0.024747237244298143\n",
      "2020-03-19 00:00:00 -0.02042824345218573\n",
      "2020-03-20 00:00:00 -0.02085559211932134\n",
      "2020-03-21 00:00:00 -0.024527988483472676\n",
      "2020-03-22 00:00:00 -0.02051001352308584\n",
      "2020-03-23 00:00:00 -0.028186658315064204\n",
      "2020-03-24 00:00:00 -0.027716994894237783\n",
      "2020-03-25 00:00:00 -0.018994855559952513\n",
      "2020-03-26 00:00:00 -0.02216216216216216\n",
      "2020-03-27 00:00:00 -0.020823139237753495\n",
      "2020-03-28 00:00:00 -0.020263238445056626\n",
      "2020-03-29 00:00:00 -0.02605824179584574\n",
      "2020-03-30 00:00:00 -0.02726346484910234\n",
      "2020-03-31 00:00:00 -0.02553847911786551\n",
      "2020-04-01 00:00:00 -0.027947424322889006\n",
      "2020-04-02 00:00:00 -0.02928377153218495\n",
      "2020-04-03 00:00:00 -0.025469195636131554\n",
      "2020-04-04 00:00:00 -0.025783118474012715\n",
      "2020-04-05 00:00:00 -0.01906861394323505\n",
      "2020-04-06 00:00:00 -0.02647319968222814\n",
      "2020-04-07 00:00:00 -0.032008140798815886\n",
      "2020-04-08 00:00:00 -0.0390625\n",
      "2020-04-09 00:00:00 -0.02547439563296075\n",
      "2020-04-10 00:00:00 -0.029648101967303962\n",
      "2020-04-11 00:00:00 -0.029054215688406586\n",
      "2020-04-12 00:00:00 -0.029634592588774933\n",
      "2020-04-13 00:00:00 -0.02657261919795068\n",
      "2020-04-14 00:00:00 -0.034302676275437316\n",
      "2020-04-15 00:00:00 -0.027899696141923207\n",
      "2020-04-16 00:00:00 -0.029374297489696517\n",
      "2020-04-17 00:00:00 -0.026157208131911795\n",
      "2020-04-18 00:00:00 -0.028705002337540905\n",
      "2020-04-19 00:00:00 -0.02896500372300819\n",
      "2020-04-20 00:00:00 -0.02572268495835375\n",
      "2020-04-21 00:00:00 -0.02696272799365583\n",
      "2020-04-22 00:00:00 -0.026032251066599177\n",
      "2020-04-23 00:00:00 -0.019546683302141817\n",
      "2020-04-24 00:00:00 -0.024892056390781875\n",
      "2020-04-25 00:00:00 -0.029911903298504405\n"
     ]
    }
   ],
   "source": [
    "dates_relevant = pd.Series(compressed_relevant.index)\n",
    "sentiments = []\n",
    "for i in range(len(compressed_relevant)):\n",
    "    docu = compressed_relevant['tweets'][i]\n",
    "\n",
    "    docu_negate, nlist = negation_handling(docu)\n",
    "    nlist_stemmed = ([stemmer.stem(w) for w in nlist])\n",
    "\n",
    "    docu_tokens = clean_tokenize(docu_negate)\n",
    "\n",
    "    docufinal = docu_tokens + nlist_stemmed\n",
    "\n",
    "    doculen = len(docufinal)\n",
    "    docu_positive = ([w for w in docufinal if w in positive_final])\n",
    "    docu_negative = ([w for w in docufinal if w in negative_final])\n",
    "    docu_sentiment = (len(docu_positive)-len(docu_negative))/doculen\n",
    "    \n",
    "    sentiments.append(docu_sentiment)\n",
    "    print(dates_relevant[i], docu_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>-0.013624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02</th>\n",
       "      <td>-0.013514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03</th>\n",
       "      <td>-0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-04</th>\n",
       "      <td>-0.002639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-05</th>\n",
       "      <td>-0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>-0.026963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-22</th>\n",
       "      <td>-0.026032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-23</th>\n",
       "      <td>-0.019547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-24</th>\n",
       "      <td>-0.024892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-25</th>\n",
       "      <td>-0.029912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentiments\n",
       "dates                 \n",
       "2020-01-01   -0.013624\n",
       "2020-01-02   -0.013514\n",
       "2020-01-03   -0.012000\n",
       "2020-01-04   -0.002639\n",
       "2020-01-05   -0.010000\n",
       "...                ...\n",
       "2020-04-21   -0.026963\n",
       "2020-04-22   -0.026032\n",
       "2020-04-23   -0.019547\n",
       "2020-04-24   -0.024892\n",
       "2020-04-25   -0.029912\n",
       "\n",
       "[116 rows x 1 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the result to a data frame and output to a csv file.\n",
    "sentiment_array = pd.DataFrame({'sentiments':sentiments},index = dates_relevant)\n",
    "sentiment_array.to_csv('Sentiments_Coronavirus.csv')\n",
    "sentiment_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
